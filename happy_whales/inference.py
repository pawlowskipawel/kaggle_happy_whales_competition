# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/inference.ipynb (unless otherwise specified).

__all__ = ['index_array_to_id_array', 'get_embeddings', 'create_and_search_index', 'create_similarity_df',
           'get_predictions', 'get_best_threshold', 'get_predictions_1_2']

# Cell
from .metrics import map_per_set
from sklearn.preprocessing import normalize
from tqdm import tqdm

import pandas as pd
import numpy as np
import torch
import faiss

# Cell
def index_array_to_id_array(array, labels):
    return labels[array]

# Cell
@torch.inference_mode()
def get_embeddings(model, dataloader, device, normalization=True, norm="l2", label_encoder=None):

    labels_list = []
    embeddings_list = []
    image_names_list = []

    with tqdm(enumerate(dataloader, 1), total=len(dataloader), unit="batch") as progress_bar:
        for step, batch in progress_bar:
            images = batch['image'].to(device)
            labels = batch['label'].to(device)
            image_names = batch['image_name']

            outputs = model(images, return_embeddings=True)
            embeddings = outputs["embeddings"]

            labels_list.append(labels.cpu().numpy())
            embeddings_list.append(embeddings.cpu().numpy())
            image_names_list.extend(image_names)

        embeddings = np.vstack(embeddings_list)
        labels = np.concatenate(labels_list)

    if normalization:
        embeddings = normalize(embeddings, axis=1, norm=norm)

    if label_encoder is not None:
        labels = label_encoder.inverse_transform(labels)

    return embeddings, labels, image_names_list

# Cell
def create_and_search_index(base_embeddings, search_embeddings, k=200, embedding_dim=512):
    index = faiss.IndexFlatIP(embedding_dim)
    index.add(base_embeddings)
    D, I = index.search(search_embeddings, k=k)

    return D, I

# Cell
def create_similarity_df(indexes, similarities, image_names, labels, label_encoder):
    similarity_df = []

    for image_name, top_k_similarities, top_k_indexes in zip(image_names, similarities, indexes):
        top_k_ids = index_array_to_id_array(top_k_indexes, labels)

        top_k_similarities_df = pd.DataFrame(np.stack([top_k_ids, top_k_similarities], axis=1), columns=['id', 'similarity'])
        top_k_similarities_df['image'] = image_name

        similarity_df.append(top_k_similarities_df)

    similarity_df = pd.concat(similarity_df).reset_index(drop=True)
    similarity_df = similarity_df.groupby(['image','id'])["similarity"].max().reset_index()

    similarity_df = similarity_df.sort_values('similarity',ascending=False).reset_index(drop=True)
    similarity_df["id"] = label_encoder.inverse_transform(similarity_df["id"].astype(int))

    return similarity_df

# Cell
def get_predictions(indexes, similarities, image_names, labels, label_encoder, threshold=0.2):

    similarity_df = create_similarity_df(indexes, similarities, image_names, labels, label_encoder)

    predictions = {}

    for image, id, similarity in tqdm(similarity_df.values):

        if image not in predictions:
            predictions[image] = []

        if len(predictions[image]) == 5:
            continue

        if similarity > threshold or "new_individual" in predictions[image]:
            predictions[image].append(id)
        else:
            predictions[image].extend(["new_individual", id])


    predictions = pd.Series(predictions).reset_index()
    predictions.columns = ['image','predictions']
    predictions["predictions"] = predictions["predictions"].str.join(" ")

    return predictions

# Cell
def get_best_threshold(indexes, similarities, valid_names, train_labels, valid_labels, label_encoder_id):

    best_threshold = None
    best_score = 0

    valid_labels_encoded = label_encoder_id.inverse_transform(valid_labels)
    train_labels_encoded = label_encoder_id.inverse_transform(train_labels)

    valid_labels_encoded = np.where(pd.Series(valid_labels_encoded).isin(train_labels_encoded), valid_labels_encoded, "new_individual")


    for threshold in np.arange(0, 1, 0.05).round(2):
        threshold_scores = []

        predictions = get_predictions(indexes, similarities, valid_names, train_labels, label_encoder_id, threshold=threshold)
        predictions["predictions"] = predictions["predictions"].str.split(" ")

        for valid_name, valid_label in zip(valid_names, valid_labels_encoded):
            image_predictions = predictions[predictions["image"] == valid_name]["predictions"].values[0]

            threshold_scores.append(map_per_image(valid_label, image_predictions))

        threshold_score = np.mean(threshold_scores)

        print(f"Threshold: {threshold} - Score: {threshold_score}")

        if threshold_score > best_score:
            best_score = threshold_score
            best_threshold = threshold

    return best_threshold

# Cell
def get_predictions_1_2(similarity_df, threshold=0.2):

    predictions = {}

    for image, id, similarity in tqdm(similarity_df.values):

        if image in predictions:
            if len(predictions[image]) == 5:
                continue

            predictions[image].append(id)

        elif similarity > threshold:
            predictions[image] = [id,'new_individual']
        else:
            predictions[image] = ['new_individual',id]


    predictions = pd.Series(predictions).reset_index()
    predictions.columns = ['image','predictions']

    predictions["predictions"] = predictions["predictions"].str.join(" ")

    return predictions