# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/training.ipynb (unless otherwise specified).

__all__ = ['get_optimizer', 'get_scheduler', 'HappyWhalesTrainer']

# Cell
from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup
from .metrics import TopKAccuracy
from torch import optim
from tqdm import tqdm

import numpy as np
import torch
import wandb
import os

# Cell
def get_optimizer(optimizer_name, model, learning_rate, weight_decay):
    no_decay = ['bias', 'LayerNorm.weight', 'LayerNorm.bias', 'BatchNorm.weight', 'BatchNorm.bias']
    optimizer_grouped_parameters = [
                {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},
                {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
            ]

    if optimizer_name == "adam":
        return optim.Adam(optimizer_grouped_parameters, lr=learning_rate)
    elif optimizer_name == "adamw":
        return optim.AdamW(optimizer_grouped_parameters, lr=learning_rate)
    elif optimizer_name == "sgd":
        return optim.SGD(optimizer_grouped_parameters, lr=learning_rate)


# Cell
def get_scheduler(cfg, optimizer, dataloader_len):
    if cfg.lr_scheduler == "cosine":
        return get_cosine_schedule_with_warmup(optimizer,
                                               num_warmup_steps=dataloader_len* cfg.scheduler_warmup_epochs,
                                               num_training_steps=dataloader_len * cfg.epochs)
    elif cfg.lr_scheduler == "onecyclelr":
        steps_per_epoch = (dataloader_len // cfg.grad_accum_iter) + dataloader_len % cfg.grad_accum_iter

        return optim.lr_scheduler.OneCycleLR(optimizer,
                                             cfg.max_learning_rate,
                                             epochs=cfg.epochs,
                                             steps_per_epoch=steps_per_epoch,
                                             pct_start=cfg.scheduler_warmup_epochs / cfg.epochs,
                                             div_factor=cfg.div_factor,
                                             final_div_factor=cfg.final_div_factor)
    else:
        return None

# Cell
class HappyWhalesTrainer:
    def __init__(self, model_name, model, criterion, optimizer,
                 lr_scheduler=None, metrics_dict=None, grad_accum_iter=1,
                 wandb_log=False, criterion_species=None, device="cuda"):

        self.wandb_log = wandb_log

        self.model = model
        self.model_name = model_name

        self.criterion = criterion
        self.criterion_species = criterion_species

        self.optimizer = optimizer
        self.lr_scheduler = lr_scheduler
        self.grad_accum_iter = grad_accum_iter

        self.device = device

        self.metrics_dict = {} if metrics_dict is None else metrics_dict

    def get_postfix(self, loss, stage):
        return {
            f"{stage} loss": f"{(loss):.3f}",
            **{metric_name: f"{metric.get_metric():.3f}" for metric_name, metric in self.metrics_dict.items()}
        }

    def log_to_wandb(self, step, loss, stage, fold_i=None):
        fold = f"_fold_{fold_i}" if fold_i is not None else ""

        metrics = {
            f"{stage}/step": step,
            **{f"{stage}/{stage}_{metric_name}" + fold: metric.get_metric() for metric_name, metric in self.metrics_dict.items()},
            **{f"{stage}/{stage}_loss" + fold: loss},
        }

        if stage == "train" and self.lr_scheduler is not None:
            metrics[f"{stage}/{stage}_lr" + fold] = self.lr_scheduler.get_last_lr()[0]

        wandb.log(metrics)

    def train_one_epoch(self, epoch, dataloader, fold_i=None):
        self.model.train()

        total_steps = len(dataloader)
        total_train_loss = 0

        with tqdm(enumerate(dataloader, 1), unit="batch", total=total_steps, bar_format='{l_bar}{bar:10}{r_bar}') as progress_bar:
            progress_bar.set_description(f"Epoch {epoch+1}".ljust(25))
            for step, batch in progress_bar:

                total_train_loss += self.train_one_step(step, batch, total_steps=total_steps)

                if (step % self.grad_accum_iter == 0) or (step == total_steps):
                    current_loss = total_train_loss / (step / self.grad_accum_iter)
                    progress_bar.set_postfix(self.get_postfix(current_loss, "train"))

                    if self.wandb_log:
                        current_step = (total_steps // self.grad_accum_iter) * epoch + (step // self.grad_accum_iter)
                        current_loss = total_train_loss / (step // self.grad_accum_iter)

                        self.log_to_wandb(current_step, current_loss, "train", fold_i)

        total_train_loss /= total_steps

        return total_train_loss

    @torch.no_grad()
    def validate_one_epoch(self, epoch, dataloader, fold_i=None):
        self.model.eval()

        total_valid_loss = 0
        total_steps = len(dataloader)

        with tqdm(enumerate(dataloader, 1), unit="batch", bar_format='{l_bar}{bar:10}{r_bar}', total=total_steps) as progress_bar:
            progress_bar.set_description(f"Validation after epoch {epoch+1}".ljust(25))

            for step, batch in progress_bar:
                total_valid_loss += self.validate_one_step(batch)
                progress_bar.set_postfix(self.get_postfix(total_valid_loss / step, "valid"))

        if self.wandb_log:
            current_step = epoch
            current_loss = total_valid_loss / step
            self.log_to_wandb(current_step, current_loss, "valid", fold_i)

        total_valid_loss /= total_steps

        return total_valid_loss

    def process_batch(self, batch):
        images = batch["image"].to(self.device)
        labels = batch["label"].to(self.device)

        if self.criterion_species is not None:
            species = batch["species"].to(self.device)

            return images, labels, species

        return images, labels, None

    def train_one_step(self, step, batch, total_steps):
        images, labels, species_labels = self.process_batch(batch)

        outputs = self.model(images, return_embeddings=False)

        logits = outputs["logits"]

        batch_loss = self.criterion(logits, labels) / self.grad_accum_iter

        if self.criterion_species is not None:
            species_logits = outputs["species_logits"]
            species_loss = self.criterion_species(species_logits, species_labels)
            batch_loss += species_loss

        batch_loss.backward()

        if self.metrics_dict:
            for _, metric in self.metrics_dict.items():
                metric.update(logits, labels)

        if (step % self.grad_accum_iter == 0) or (step == total_steps):
            self.optimizer.step()

            if self.lr_scheduler is not None: self.lr_scheduler.step()

            for p in self.model.parameters():
                p.grad = None

        return batch_loss.item()

    def validate_one_step(self, batch):
        images, labels, species_labels = self.process_batch(batch)

        outputs = self.model(images, return_embeddings=False)
        logits = outputs["logits"]

        batch_loss = self.criterion(logits, labels)

        if self.criterion_species is not None:
            species_logits = outputs["species_logits"]
            species_loss = self.criterion_species(species_logits, species_labels)
            batch_loss += species_loss

        if self.metrics_dict:
            for _, metric in self.metrics_dict.items():
                metric.update(logits, labels)

        return batch_loss.item()

    def fit(self, epochs, train_dataloader, valid_dataloader, save_path="trained_models", fold_i=None):

        if save_path:
            save_path += f"/{self.model_name}"

            if not os.path.exists(save_path):
                os.makedirs(save_path)

            save_path += f"/fold{fold_i}_best_state_dict.pth"

        best_valid_loss = np.inf
        best_accuracy = 0

        for epoch in range(epochs):
            epoch_loss = self.train_one_epoch(epoch, train_dataloader, fold_i)

            if self.metrics_dict:
                for _, metric in self.metrics_dict.items():
                    metric.reset()

            valid_loss = self.validate_one_epoch(epoch, valid_dataloader, fold_i)

            if save_path:
                if valid_loss < best_valid_loss:
                    best_valid_loss = valid_loss
                    torch.save(self.model.state_dict(), save_path)

            if self.metrics_dict:
                for _, metric in self.metrics_dict.items():
                    metric.reset()

        return best_valid_loss